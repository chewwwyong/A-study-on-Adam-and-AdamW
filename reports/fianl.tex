\documentclass[14pt,twocolumn,letterpaper]{extarticle}


\setlength{\textheight}{9.075in}
\setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in}
\setlength{\topmargin}{-0.3in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parindent}{1pc}
\setlength{\oddsidemargin}{-.304in}
\setlength{\evensidemargin}{-.304in}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{courier}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{booktabs}

\usepackage{indentfirst}
\setlength{\parindent}{1em}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\setcounter{page}{1}
\begin{document}

\title{A Study On Adam And AdamW}
\author{D11949006 Wei-Lun Chen \\ R11921118 Yong-Tai Qiu}
\date{}
\maketitle

\section{Introduction}
The field of machine learning has witnessed the advent of numerous optimization algorithms, among which the Adam and AdamW optimizers stand out due to their unique characteristics and widespread use. Despite their extensive application, certain features within these optimizers still call for comprehensive exploration and validation. Therefore, this research sets out to achieve three primary objectives centered around these optimizers.\par
Firstly,we strive to gain a comprehensive understanding of the Adam and AdamW optimizers. We seek to delve into their operational principles, understand the theoretical underpinnings that govern their functioning, and explore their common usage in machine learning tasks. This foundational understanding will set the stage for subsequent investigations and pave the way for our next objectives.\par
Our second goal is to evaluate the significance of the bias correction feature intrinsic to the Adam optimizer. This bias correction, integral to Adam's functionality, is designed to offset any underestimation of the gradient moments during the initial stages of training. However, the practical implications of this feature on model performance are not fully understood. We propose to probe into the role of bias correction through extensive experimentation and detailed analysis. Our study design includes varying learning rates and beta values while observing the impacts with and without bias correction.\par
Finally, our third aim is to draw comparisons between L2 regularization in Adam and weight decay in AdamW. AdamW was conceived as a solution to the issues found with weight decay in Adam, yet the distinction between the two and their consequent impact on model performance remain somewhat hazy. Our research intends to illuminate these subtle differences, offering strong empirical evidence to validate the distinction. This objective includes experiments with varying lambda values to understand the effects on L2 regularization and AdamW.\par
Through this study, we aspire to enrich our understanding of these optimizers and offer significant insights that may inform optimization decisions in machine learning projects. Our experimental outcomes, documented through loss and accuracy measures, will provide robust evidence to support our conclusions.

\section{Methods}
In our research, we conducted a comparative evaluation using a Convolutional Neural Network (CNN) model performing classification tasks on the MNIST dataset. To maintain a balanced comparison, we kept consistent the model architecture, initial weight settings, training epochs, batch size, and the loss function across all experimental setups. We scrutinized four unique settings of the optimizer: the conventional Adam optimizer, a modified Adam version without bias correction, Adam incorporating L2 regularization, and AdamW. These configurations were meticulously chosen to explore the impacts of bias correction and different weight decay methods on the performance of the model.\par
For every optimizer setup, we trained our CNN model across 100 epochs with a batch size of 64. To ensure consistency in the starting point, we initiated the model with identical random seeds across all conditions, guaranteeing uniform initial weights. The loss function employed was Cross Entropy Loss.\par
We undertook the training with three distinct learning rates $\left(\gamma\right)$ – 1e-3, 1e-4, and 1e-5 – applied to each of the four optimizers. This procedure enabled us to study the variation in the model's performance under different learning rates spanning the optimizers. In all scenarios, the beta parameters $\left(\beta_{1}\ \text{and}\ \beta_{2}\right)$ were set at 0.9 and 0.999, respectively, while the epsilon $\left(\epsilon\right)$ was established at 1e-8. These configurations were selected to be in sync with commonly used values in existing literature.\par
Finally, we integrated weight decay in the Adam with L2 regularization and the AdamW optimizers, standardizing the decay factor $\left(\lambda\right)$ to 0.01 in both instances. This facilitated us to draw direct comparisons between the effects of L2 regularization and weight decay decoupling on the model's performance.\par
Through this experimental design, we were able to systematically investigate the comparative impacts of bias correction and different weight decay methods on the performance of a CNN model in image classification tasks.

\begin{table*}[ht!]
\centering
\caption{Experimental Settings}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}p{.35\textwidth}}
\begin{tabularx}{\textwidth}{|Y|Z|Z|}
\hline
\textbf{Hyperparameter} & \textbf{Exp1} & \textbf{Exp2} \\
\hline
Optimizer & \makecell{Adam \\ Adam w/o Bias Correction} & \makecell{Adam (L2) \\ AdamW} \\
Learning Rate ($\gamma$) & 1e-3, 1e-4, 1e-5 & 1e-4 \\
\makecell{Beta Parameters \\ ($\beta_1$, $\beta_2$)} & (0.9, 0.999), (0.99, 0.999) & (0.9, 0.999) \\
Epsilon ($\epsilon$) & 1e-8 & 1e-8 \\
Decay Factor ($\lambda$) & - & 1e-2, 1e-3, 1e-4 \\
\hline
\end{tabularx}
\label{tab:exp_settings}
\end{table*}

\section{Results}

\section{Discussion and Conclusions}


\bibliographystyle{ieee}
% \bibliography{egbib}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
